"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const __1 = require("./..");
const assertable_1 = require("./../utils/assertable");
class FNNModel extends assertable_1.Assertable {
    constructor(opt) {
        super();
        this.model = { hidden: { Wh: [], bh: [] }, decoder: { Wh: null, b: null } };
        this.initializeNeuralNetworkFromGivenOptions(opt);
    }
    initializeNeuralNetworkFromGivenOptions(opt) {
        this.graph = new __1.Graph();
        if (FNNModel.isFromJSON(opt)) {
            this.initializeModelFromJSONObject(opt);
        }
        else if (FNNModel.isFreshInstanceCall(opt)) {
            this.initializeModelAsFreshInstance(opt);
        }
        else {
            FNNModel.assert(false, 'Improper input for DNN.');
        }
    }
    static isFromJSON(opt) {
        return FNNModel.has(opt, ['hidden', 'decoder'])
            && FNNModel.has(opt.hidden, ['Wh', 'bh'])
            && FNNModel.has(opt.decoder, ['Wh', 'b']);
    }
    initializeModelFromJSONObject(opt) {
        this.initializeHiddenLayerFromJSON(opt);
        this.model.decoder.Wh = __1.Mat.fromJSON(opt['decoder']['Wh']);
        this.model.decoder.b = __1.Mat.fromJSON(opt['decoder']['b']);
    }
    initializeHiddenLayerFromJSON(opt) {
        FNNModel.assert(!Array.isArray(opt['hidden']['Wh']), 'Wrong JSON Format to recreate Hidden Layer.');
        for (let i = 0; i < opt.hidden.Wh.length; i++) {
            this.model.hidden.Wh[i] = __1.Mat.fromJSON(opt.hidden.Wh[i]);
            this.model.hidden.bh[i] = __1.Mat.fromJSON(opt.hidden.bh[i]);
        }
    }
    static isFreshInstanceCall(opt) {
        return FNNModel.has(opt, ['architecture']) && FNNModel.has(opt.architecture, ['inputSize', 'hiddenUnits', 'outputSize']);
    }
    initializeModelAsFreshInstance(opt) {
        this.architecture = this.determineArchitectureProperties(opt);
        this.training = this.determineTrainingProperties(opt);
        const mu = opt['mu'] ? opt['mu'] : 0;
        const std = opt['std'] ? opt['std'] : 0.1;
        this.model = this.initializeFreshNetworkModel();
        this.initializeHiddenLayer(mu, std);
        this.initializeDecoder(mu, std);
    }
    determineArchitectureProperties(opt) {
        const out = { inputSize: null, hiddenUnits: null, outputSize: null };
        out.inputSize = typeof opt.architecture.inputSize === 'number' ? opt.architecture.inputSize : 1;
        out.hiddenUnits = Array.isArray(opt.architecture.hiddenUnits) ? opt.architecture.hiddenUnits : [1];
        out.outputSize = typeof opt.architecture.outputSize === 'number' ? opt.architecture.outputSize : 1;
        return out;
    }
    determineTrainingProperties(opt) {
        const out = { alpha: null, lossClamp: null, loss: null };
        if (!opt.training) {
            opt.training = out;
        }
        out.alpha = typeof opt.training.alpha === 'number' ? opt.training.alpha : 0.01;
        out.lossClamp = typeof opt.training.lossClamp === 'number' ? opt.training.lossClamp : 1;
        out.loss = typeof opt.training.loss === 'number' ? opt.training.loss : 1e-6;
        return out;
    }
    initializeFreshNetworkModel() {
        return {
            hidden: {
                Wh: new Array(this.architecture.hiddenUnits.length),
                bh: new Array(this.architecture.hiddenUnits.length)
            },
            decoder: {
                Wh: null,
                b: null
            }
        };
    }
    initializeHiddenLayer(mu, std) {
        let hiddenSize;
        for (let i = 0; i < this.architecture.hiddenUnits.length; i++) {
            const previousSize = this.getPrecedingLayerSize(i);
            hiddenSize = this.architecture.hiddenUnits[i];
            this.model.hidden.Wh[i] = new __1.RandMat(hiddenSize, previousSize, mu, std);
            this.model.hidden.bh[i] = new __1.Mat(hiddenSize, 1);
        }
    }
    getPrecedingLayerSize(i) {
        return i === 0 ? this.architecture.inputSize : this.architecture.hiddenUnits[i - 1];
    }
    initializeDecoder(mu, std) {
        this.model.decoder.Wh = new __1.RandMat(this.architecture.outputSize, this.architecture.hiddenUnits[this.architecture.hiddenUnits.length - 1], mu, std);
        this.model.decoder.b = new __1.Mat(this.architecture.outputSize, 1);
    }
    setTrainability(isTrainable) {
        this.graph.forgetCurrentSequence();
        this.graph.memorizeOperationSequence(isTrainable);
    }
    backward(expectedOutput, alpha) {
        FNNModel.assert(this.graph.isMemorizingSequence(), '[' + this.constructor.name + '] Trainability is not enabled.');
        FNNModel.assert(typeof this.previousOutput !== 'undefined', '[' + this.constructor.name + '] Please execute `forward()` before calling `backward()`');
        this.propagateLossIntoDecoderLayer(expectedOutput);
        this.backwardGraph();
        this.updateWeights(alpha);
        this.resetGraph();
    }
    backwardGraph() {
        this.graph.backward();
    }
    resetGraph() {
        this.graph.forgetCurrentSequence();
    }
    propagateLossIntoDecoderLayer(expected) {
        let loss;
        for (let i = 0; i < this.architecture.outputSize; i++) {
            loss = this.previousOutput.w[i] - expected[i];
            if (Math.abs(loss) <= this.training.loss) {
                continue;
            }
            else {
                loss = this.clipLoss(loss);
                this.previousOutput.dw[i] = loss;
            }
        }
    }
    clipLoss(loss) {
        if (loss > this.training.lossClamp) {
            return this.training.lossClamp;
        }
        else if (loss < -this.training.lossClamp) {
            return -this.training.lossClamp;
        }
        return loss;
    }
    updateWeights(alpha) {
        alpha = alpha ? alpha : this.training.alpha;
        this.updateHiddenLayer(alpha);
        this.updateDecoderLayer(alpha);
    }
    updateHiddenLayer(alpha) {
        for (let i = 0; i < this.architecture.hiddenUnits.length; i++) {
            this.model.hidden.Wh[i].update(alpha);
            this.model.hidden.bh[i].update(alpha);
        }
    }
    updateDecoderLayer(alpha) {
        this.model.decoder.Wh.update(alpha);
        this.model.decoder.b.update(alpha);
    }
    forward(input) {
        const mat = this.transformArrayToMat(input);
        const activations = this.specificForwardpass(mat);
        const outputMat = this.computeOutput(activations);
        const output = this.transformMatToArray(outputMat);
        this.previousOutput = outputMat;
        return output;
    }
    transformArrayToMat(input) {
        const mat = new __1.Mat(this.architecture.inputSize, 1);
        mat.setFrom(input);
        return mat;
    }
    transformMatToArray(input) {
        const arr = input.w.slice(0);
        return arr;
    }
    computeOutput(hiddenUnitActivations) {
        const weightedInputs = this.graph.mul(this.model.decoder.Wh, hiddenUnitActivations[hiddenUnitActivations.length - 1]);
        return this.graph.add(weightedInputs, this.model.decoder.b);
    }
    getSquaredLossFor(input, expectedOutput) {
        const trainability = this.graph.isMemorizingSequence();
        this.setTrainability(false);
        const lossSum = this.calculateLossSumByForwardPass(input, expectedOutput);
        this.setTrainability(trainability);
        return lossSum * lossSum;
    }
    calculateLossSumByForwardPass(input, expected) {
        let lossSum = 0;
        const actualOutput = this.forward(input);
        for (let i = 0; i < this.architecture.outputSize; i++) {
            const loss = actualOutput[i] - expected[i];
            lossSum += loss;
        }
        return lossSum;
    }
    static has(obj, keys) {
        FNNModel.assert(obj, 'Improper input for DNN.');
        for (const key of keys) {
            if (Object.hasOwnProperty.call(obj, key)) {
                continue;
            }
            return false;
        }
        return true;
    }
}
exports.FNNModel = FNNModel;
//# sourceMappingURL=fnn-model.js.map