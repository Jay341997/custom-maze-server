"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const recurrent_js_1 = require("recurrent-js");
const _1 = require("./../.");
class DQNSolver extends _1.Solver {
    constructor(env, opt) {
        super(env, opt);
        this.shortTermMemory = { s0: null, a0: null, r0: null, s1: null, a1: null };
        this.numberOfHiddenUnits = opt.get('numberOfHiddenUnits');
        this.epsilonMax = opt.get('epsilonMax');
        this.epsilonMin = opt.get('epsilonMin');
        this.epsilonDecayPeriod = opt.get('epsilonDecayPeriod');
        this.epsilon = opt.get('epsilon');
        this.experienceSize = opt.get('experienceSize');
        this.gamma = opt.get('gamma');
        this.alpha = opt.get('alpha');
        this.doLossClipping = opt.get('doLossClipping');
        this.lossClamp = opt.get('lossClamp');
        this.doRewardClipping = opt.get('doRewardClipping');
        this.rewardClamp = opt.get('rewardClamp');
        this.keepExperienceInterval = opt.get('keepExperienceInterval');
        this.replaySteps = opt.get('replaySteps');
        this.isInTrainingMode = opt.get('trainingMode');
        this.reset();
    }
    reset() {
        this.numberOfHiddenUnits = this.opt.get('numberOfHiddenUnits');
        this.numberOfStates = this.env.get('numberOfStates');
        this.numberOfActions = this.env.get('numberOfActions');
        const netOpts = {
            architecture: {
                inputSize: this.numberOfStates,
                hiddenUnits: this.numberOfHiddenUnits,
                outputSize: this.numberOfActions
            }
        };
        this.net = new recurrent_js_1.Net(netOpts);
        this.learnTick = 0;
        this.memoryIndexTick = 0;
        this.shortTermMemory.s0 = null;
        this.shortTermMemory.a0 = null;
        this.shortTermMemory.r0 = null;
        this.shortTermMemory.s1 = null;
        this.shortTermMemory.a1 = null;
        this.longTermMemory = [];
    }
    setTrainingModeTo(trainingMode) {
        this.isInTrainingMode = trainingMode;
    }
    getTrainingMode() {
        return this.isInTrainingMode;
    }
    toJSON() {
        const j = {
            ns: this.numberOfStates,
            nh: this.numberOfHiddenUnits,
            na: this.numberOfActions,
            net: recurrent_js_1.Net.toJSON(this.net)
        };
        return j;
    }
    fromJSON(json) {
        this.numberOfStates = json.ns;
        this.numberOfHiddenUnits = json.nh;
        this.numberOfActions = json.na;
        this.net = recurrent_js_1.Net.fromJSON(json.net);
    }
    decide(state) {
        const stateVector = new recurrent_js_1.Mat(this.numberOfStates, 1);
        stateVector.setFrom(state);
        const actionIndex = this.epsilonGreedyActionPolicy(stateVector);
        this.shiftStateMemory(stateVector, actionIndex);
        return actionIndex;
    }
    epsilonGreedyActionPolicy(stateVector) {
        let actionIndex = 0;
        if (Math.random() < this.currentEpsilon()) {
            actionIndex = recurrent_js_1.Utils.randi(0, this.numberOfActions);
        }
        else {
            const actionVector = this.forwardQ(stateVector);
            actionIndex = recurrent_js_1.Utils.argmax(actionVector.w);
        }
        return actionIndex;
    }
    currentEpsilon() {
        if (this.isInTrainingMode) {
            if (this.learnTick < this.epsilonDecayPeriod) {
                return this.epsilonMax - (this.epsilonMax - this.epsilonMin) / this.epsilonDecayPeriod * this.learnTick;
            }
            else {
                return this.epsilonMin;
            }
        }
        else {
            return this.epsilon;
        }
    }
    forwardQ(stateVector) {
        const graph = new recurrent_js_1.Graph();
        const a2Mat = this.determineActionVector(graph, stateVector);
        return a2Mat;
    }
    backwardQ(stateVector) {
        const graph = new recurrent_js_1.Graph();
        graph.memorizeOperationSequence(true);
        const a2Mat = this.determineActionVector(graph, stateVector);
        return a2Mat;
    }
    determineActionVector(graph, stateVector) {
        const a2mat = this.net.forward(stateVector, graph);
        this.backupGraph(graph);
        return a2mat;
    }
    backupGraph(graph) {
        this.previousGraph = graph;
    }
    shiftStateMemory(stateVector, actionIndex) {
        this.shortTermMemory.s0 = this.shortTermMemory.s1;
        this.shortTermMemory.a0 = this.shortTermMemory.a1;
        this.shortTermMemory.s1 = stateVector;
        this.shortTermMemory.a1 = actionIndex;
    }
    learn(r) {
        if (this.shortTermMemory.r0 && this.alpha > 0) {
            this.learnFromSarsaTuple(this.shortTermMemory);
            this.addToReplayMemory();
            this.limitedSampledReplayLearning();
        }
        this.shiftRewardIntoMemory(r);
    }
    shiftRewardIntoMemory(r) {
        this.shortTermMemory.r0 = this.clipReward(r);
    }
    clipReward(r) {
        return this.doRewardClipping ? Math.sign(r) * Math.min(Math.abs(r), this.rewardClamp) : r;
    }
    learnFromSarsaTuple(sarsa) {
        const q1Max = this.getTargetQ(sarsa.s1, sarsa.r0);
        const q0ActionVector = this.backwardQ(sarsa.s0);
        const q0Max = q0ActionVector.w[sarsa.a0];
        let loss = q0Max - q1Max;
        loss = this.clipLoss(loss);
        q0ActionVector.dw[sarsa.a0] = loss;
        this.previousGraph.backward();
        this.net.update(this.alpha);
    }
    getTargetQ(s1, r0) {
        const targetActionVector = this.forwardQ(s1);
        const targetActionIndex = recurrent_js_1.Utils.argmax(targetActionVector.w);
        const qMax = r0 + this.gamma * targetActionVector.w[targetActionIndex];
        return qMax;
    }
    clipLoss(loss) {
        if (this.doLossClipping) {
            if (loss > this.lossClamp) {
                loss = this.lossClamp;
            }
            else if (loss < -this.lossClamp) {
                loss = -this.lossClamp;
            }
        }
        return loss;
    }
    addToReplayMemory() {
        if (this.learnTick % this.keepExperienceInterval === 0) {
            this.addShortTermToLongTermMemory();
        }
        this.learnTick++;
    }
    addShortTermToLongTermMemory() {
        const sarsa = this.extractSarsaExperience();
        this.longTermMemory[this.memoryIndexTick] = sarsa;
        this.memoryIndexTick++;
        if (this.memoryIndexTick > this.experienceSize - 1) {
            this.memoryIndexTick = 0;
        }
    }
    extractSarsaExperience() {
        const s0 = new recurrent_js_1.Mat(this.shortTermMemory.s0.rows, this.shortTermMemory.s0.cols);
        s0.setFrom(this.shortTermMemory.s0.w);
        const s1 = new recurrent_js_1.Mat(this.shortTermMemory.s1.rows, this.shortTermMemory.s1.cols);
        s1.setFrom(this.shortTermMemory.s1.w);
        const sarsa = {
            s0,
            a0: this.shortTermMemory.a0,
            r0: this.shortTermMemory.r0,
            s1,
            a1: this.shortTermMemory.a1
        };
        return sarsa;
    }
    limitedSampledReplayLearning() {
        for (let i = 0; i < this.replaySteps; i++) {
            const ri = recurrent_js_1.Utils.randi(0, this.longTermMemory.length);
            const sarsa = this.longTermMemory[ri];
            this.learnFromSarsaTuple(sarsa);
        }
    }
}
exports.DQNSolver = DQNSolver;
//# sourceMappingURL=dqn-solver.js.map